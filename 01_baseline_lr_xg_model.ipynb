{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Baseline xG (Goal vs Non-Goal) \u2014 Skeleton\n\nWe'll load ../data/xgoal-db.sqlite if present, build simple geometry/context features, train a calibrated Logistic Regression, and produce basic metrics + explanations. Non-penalty, non-own-goal only. Everything is guarded so the notebook runs even when the DB is missing.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Core\nimport os, sqlite3, math, json\nfrom pathlib import Path\n\n# Data\nimport numpy as np\nimport pandas as pd\n\n# Modeling\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import roc_auc_score, log_loss, brier_score_loss\n\n# Plots\nimport matplotlib.pyplot as plt\n\n# Repro\nSEED = 42\nnp.random.seed(SEED)\n\n# ---- Config ----\nDB_PATH = Path(\"../data/xgoal-db.sqlite\")   # <\u2014 YOUR DB path\nUSE_MATERIALIZED_WIDE = True                # if table shots_wide exists, use it\nPRINT_ROWS = 5\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "WIDE_CTE_SQL = \"\"\"\nWITH ff AS (\n    SELECT\n        shot_id,\n        COUNT(*)                                                AS ff_count,\n        SUM(CASE WHEN teammate = 1 THEN 1 ELSE 0 END)          AS ff_teammates,\n        SUM(CASE WHEN teammate = 0 THEN 1 ELSE 0 END)          AS ff_opponents,\n        SUM(CASE WHEN keeper   = 1 THEN 1 ELSE 0 END)          AS ff_keeper_count,\n        AVG(CASE WHEN keeper   = 1 THEN x END)                 AS ff_keeper_x,\n        AVG(CASE WHEN keeper   = 1 THEN y END)                 AS ff_keeper_y\n    FROM freeze_frames\n    GROUP BY shot_id\n)\nSELECT\n    s.*,\n    e.under_pressure       AS event_under_pressure,\n    e.counterpress         AS event_counterpress,\n    e.duration             AS event_duration,\n    ff.ff_count,\n    ff.ff_teammates,\n    ff.ff_opponents,\n    ff.ff_keeper_count,\n    ff.ff_keeper_x,\n    ff.ff_keeper_y\nFROM shots s\nLEFT JOIN events e ON e.event_id = s.shot_id\nLEFT JOIN ff     ON ff.shot_id   = s.shot_id;\n\"\"\"\n\ndef load_wide_df(db_path: Path, use_materialized: bool = True) -> pd.DataFrame | None:\n    if not db_path.exists():\n        print(f\"[info] DB not found at {db_path.resolve()}. Proceeding with skeleton only.\")\n        return None\n    with sqlite3.connect(db_path) as conn:\n        tables = set(pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)[\"name\"])\n        if \"shots_wide\" in tables and use_materialized:\n            df = pd.read_sql(\"SELECT * FROM shots_wide;\", conn)\n        else:\n            df = pd.read_sql(WIDE_CTE_SQL, conn)\n    if not df.empty and not df[\"shot_id\"].is_unique:\n        raise ValueError(\"shot_id must be unique (one row per shot)\")\n    return df\n\ndf = load_wide_df(DB_PATH, USE_MATERIALIZED_WIDE)\nprint(\"[info] df is\", None if df is None else df.shape)\nif df is not None:\n    display(df.head(PRINT_ROWS))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if df is None or df.empty:\n    print(\"[warn] No data loaded. Creating a tiny mock frame so the notebook runs end-to-end.\")\n    df = pd.DataFrame({\n        \"shot_id\": [\"a\",\"b\",\"c\"],\n        \"outcome\": [\"Goal\",\"Saved\",\"Miss\"],\n        \"is_penalty\": [False, False, False],\n        \"is_own_goal\": [False, False, False],\n        \"start_x\": [100.0, 90.0, 80.0],\n        \"start_y\": [40.0, 30.0, 50.0],\n        \"body_part\": [\"Right Foot\",\"Head\",\"Left Foot\"],\n        \"is_set_piece\": [False, False, True],\n        \"is_corner\": [False, False, False],\n        \"is_free_kick\": [False, False, True],\n        \"first_time\": [True, False, False],\n        \"under_pressure\": [False, True, False],\n        \"ff_opponents\": [1.0, 3.0, 0.0],\n        \"ff_keeper_x\": [118.0, 119.0, 117.5],\n        \"ff_keeper_y\": [40.0, 41.0, 39.5],\n        \"match_id\": [1,1,1],\n    })\n\ndf0 = df.copy()\nmask_valid = (~df0[\"is_penalty\"].astype(bool)) & (~df0[\"is_own_goal\"].astype(bool))\ndata = df0.loc[mask_valid].copy()\n\ny = (data[\"outcome\"] == \"Goal\").astype(int)\nLEAKY = [c for c in [\"end_x\",\"end_y\",\"end_z\"] if c in data.columns]\ndata = data.drop(columns=LEAKY, errors=\"ignore\")\nprint(\"rows:\", len(data), \"goals:\", int(y.sum()), \"base rate:\", float(y.mean()))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "SB_LEN, SB_WID = 120.0, 80.0\nGOAL_YC_SB = 40.0\nGOAL_HALF_W_SB = (7.32/2) * (SB_WID / 68.0)  # ~3.66m scaled to SB units\n\ndef distance_sb(x, y):\n    dx = SB_LEN - x\n    dy = GOAL_YC_SB - y\n    return np.hypot(dx, dy)\n\ndef opening_angle_deg_sb(x, y):\n    left  = np.array([SB_LEN, GOAL_YC_SB - GOAL_HALF_W_SB])\n    right = np.array([SB_LEN, GOAL_YC_SB + GOAL_HALF_W_SB])\n    p = np.column_stack([x, y])\n    v1 = left  - p\n    v2 = right - p\n    dot = (v1 * v2).sum(axis=1)\n    n1 = np.linalg.norm(v1, axis=1)\n    n2 = np.linalg.norm(v2, axis=1)\n    cosang = np.clip(dot/(n1*n2), -1, 1)\n    return np.degrees(np.arccos(cosang))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "X = pd.DataFrame(index=data.index)\nX[\"dist_sb\"] = distance_sb(data[\"start_x\"].to_numpy(), data[\"start_y\"].to_numpy())\nX[\"angle_deg_sb\"] = opening_angle_deg_sb(data[\"start_x\"].to_numpy(), data[\"start_y\"].to_numpy())\n\ndef to_num(s): return s.fillna(False).astype(int) if s.dtype != float else s.fillna(0)\n\nfor col in [\"is_set_piece\",\"is_corner\",\"is_free_kick\",\"first_time\",\"under_pressure\"]:\n    if col in data.columns:\n        X[col] = to_num(data[col].astype(\"boolean\") if col in data.columns else pd.Series(False, index=data.index))\n    else:\n        X[col] = 0\n\nX[\"is_header\"] = (data.get(\"body_part\",\"\") == \"Head\").astype(int) if \"body_part\" in data.columns else 0\n\nif {\"ff_keeper_x\",\"ff_keeper_y\"}.issubset(data.columns):\n    X[\"gk_depth_sb\"] = np.maximum(0.0, SB_LEN - data[\"ff_keeper_x\"])\n    X[\"gk_offset_sb\"] = data[\"ff_keeper_y\"] - GOAL_YC_SB\nelse:\n    X[\"gk_depth_sb\"] = 0.0\n    X[\"gk_offset_sb\"] = 0.0\n\nX[\"ff_opponents\"] = data[\"ff_opponents\"].fillna(0) if \"ff_opponents\" in data.columns else 0\ndisplay(X.describe().T.head(10))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def grouped_split(X, y, groups):\n    if groups is None or groups.isna().all() or groups.nunique() < 2:\n        # fallback: simple split by index\n        idx = np.arange(len(X))\n        np.random.shuffle(idx)\n        cut = int(0.8*len(idx))\n        tr, te = idx[:cut], idx[cut:]\n        return tr, te\n    gkf = GroupKFold(n_splits=5)\n    tr, te = next(gkf.split(X, y, groups))\n    return tr, te\n\ngroups = data[\"match_id\"] if \"match_id\" in data.columns else None\ntrain_idx, test_idx = grouped_split(X, y, groups)\n\nX_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]\ny_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]\nprint(X_tr.shape, X_te.shape, float(y_tr.mean()), float(y_te.mean()))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def safe_metrics(y_true, p):\n    if len(np.unique(y_true)) < 2:\n        return {\"auc\": np.nan, \"logloss\": np.nan, \"brier\": np.nan}\n    return {\n        \"auc\": float(roc_auc_score(y_true, p)),\n        \"logloss\": float(log_loss(y_true, p)),\n        \"brier\": float(brier_score_loss(y_true, p))\n    }\n\nbase = make_pipeline(\n    StandardScaler(with_mean=True, with_std=True),\n    LogisticRegression(max_iter=1000, solver=\"lbfgs\", C=1.0)\n)\nbase.fit(X_tr, y_tr)\np_raw = base.predict_proba(X_te)[:,1]\nprint(\"Uncalibrated:\", safe_metrics(y_te, p_raw))\n\ncal = CalibratedClassifierCV(base, method=\"sigmoid\", cv=\"prefit\")\ncal.fit(X_tr, y_tr)\np_cal = cal.predict_proba(X_te)[:,1]\nprint(\"Calibrated:\", safe_metrics(y_te, p_cal))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.calibration import calibration_curve\n\nif len(X_te) > 50 and len(np.unique(y_te)) == 2:\n    prob_true, prob_pred = calibration_curve(y_te, p_cal, n_bins=10, strategy=\"quantile\")\n    plt.figure()\n    plt.plot([0,1],[0,1], \"--\", label=\"perfect\")\n    plt.plot(prob_pred, prob_true, marker=\"o\", label=\"calibrated\")\n    plt.xlabel(\"Predicted probability\")\n    plt.ylabel(\"Observed frequency\")\n    plt.title(\"Reliability \u2014 Calibrated LR\")\n    plt.legend(); plt.show()\nelse:\n    print(\"[info] Skipping reliability plot (not enough test data).\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "lr = cal.base_estimator.named_steps[\"logisticregression\"]\ncoef = lr.coef_[0]\nfeat_names = X_tr.columns\ncoef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coef}).sort_values(\"coef\", ascending=False)\ndisplay(coef_df.head(10)); display(coef_df.tail(10))\n\ndef distance_phrase(d):\n    return \"close range\" if d < 12 else \"medium range\" if d < 18 else \"long range\"\ndef angle_phrase(a):\n    return \"wide angle\" if a > 25 else \"moderate angle\" if a > 10 else \"tight angle\"\ndef defenders_phrase(n):\n    if n >= 3:\n        return \"crowded sightline\"\n    if n == 0:\n        return \"clear sight\"\n    return None\ndef gk_phrase(depth, offset):\n    parts = []\n    if depth >= 6:\n        parts.append(\"deep GK\")\n    if abs(offset) >= 2.5:\n        parts.append(\"GK off-center\")\n    return \"; \".join(parts) if parts else None\n\ndef explain_row(r: pd.Series, xg: float) -> str:\n    bits = [f\"{xg:.2f} xG \u2014 {distance_phrase(r['dist_sb'])}, {angle_phrase(r['angle_deg_sb'])}\"]\n    if r.get(\"first_time\", 0) == 1:\n        bits.append(\"first-time\")\n    if r.get(\"is_free_kick\", 0) == 1:\n        bits.append(\"direct free-kick\")\n    dp = defenders_phrase(int(r.get(\"ff_opponents\", 0)))\n    if dp:\n        bits.append(dp)\n    gp = gk_phrase(r.get(\"gk_depth_sb\", 0.0), r.get(\"gk_offset_sb\", 0.0))\n    if gp:\n        bits.append(gp)\n    return \", \".join(bits) + \".\"\n\n# demo for a few test rows (guarded)\nn_demo = min(5, len(X_te))\nif n_demo > 0:\n    demo = X_te.head(n_demo)\n    px = cal.predict_proba(demo)[:,1]\n    for r, p in zip(demo.itertuples(index=False), px):\n        print(explain_row(pd.Series(r._asdict(), index=demo.columns), float(p)))\nelse:\n    print(\"[info] No test rows to demo explanations.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pathlib import Path\nimport joblib, json\n\nART_DIR = Path(\"artifacts\"); ART_DIR.mkdir(exist_ok=True, parents=True)\n\nif len(X_tr) > 0:\n    joblib.dump(cal, ART_DIR / \"baseline_lr_calibrated.joblib\")\n    with open(ART_DIR / \"feature_names.json\", \"w\") as f:\n        json.dump(list(X.columns), f, indent=2)\n    with open(ART_DIR / \"README.txt\", \"w\") as f:\n        f.write(\n            \"Baseline calibrated Logistic Regression for non-penalty shots.\\n\"\n            \"Inputs: features in feature_names.json\\n\"\n            \"Target: outcome == 'Goal'\\n\"\n            \"Calibration: Platt (sigmoid) on train fold\\n\"\n        )\n    print(\"Saved artifacts to:\", ART_DIR.resolve())\nelse:\n    print(\"[info] Skipping artifact save (no training data).\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Next steps\n\n- Add defenders-in-cone & nearest-defender distance.\n- Try isotonic calibration vs sigmoid.\n- Add LightGBM (shallow) and compare Brier/LogLoss.\n- Ensure GroupKFold across all folds for stable metrics.\n- Plug model + explanation into FastAPI/Streamlit.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}